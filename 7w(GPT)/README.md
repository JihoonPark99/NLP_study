7w NLP
1. GPT1

----
> - **GPT1_세미나pdf 참조** : 자세한설명
> - GPT1_전체적인 내용_손필기  : 글씨체가 예쁘진 않음. 우리 2조의 이해를 크게 도왔던 파일
> - [벡터단위로 세세하게 나타내준 블로그: 감사합니당~](https://velog.io/@gypsi12/GPT-1Improving-languague-understanding-by-Generative-Pre-Training%EB%9E%80%EB%B2%A1%ED%84%B0-%ED%9D%90%EB%A6%84-%ED%95%98%EB%82%98%ED%95%98%EB%82%98-%EC%9E%90%EC%84%B8%ED%95%98%EA%B2%8C-%EC%84%A4%EB%AA%85)

----

- 1. NLP 모델들의 역사
> RNN(1986) → LSTM(1997) → Seq2Seq(2014) → Attention(2015) → Transformer(2017)
- 2. BERT와 GPT의 형식적 차이

- 3. Prerequisite Requirement For GPT
	- a. Language Model
	- b. Generative Model & Discriminative Model
	- c. Supervised Learning & Unsupervised Learning
	- d. Pre-Training & Fine-Tuning
	- e. Auxiliary
	- f. BPE(Byte-Per-Encoding)
	
- 4. Model Description
	- a. Model Architecture
	- b. Pre-Training
	- c. Supervised Fine-Tuning
	- d. Auxiliary Objectives
	- e. Experiments
	
- 5. 추가적인 실험 [Analysis]
	- a. Pre-Training의 층이 많을수록 효과적일까?
	- b. Zero-shot Behaviors
	
- 6. GPT-1의 한계와 의의
- 7. GPT-2
- 8. GPT-3

